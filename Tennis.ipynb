{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de jugadores de tenis y verificación de zona de servicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción de las librerías utilizadas\n",
    "\n",
    "Este bloque de código importa diversas bibliotecas estándar y especializadas para el procesamiento de videos, detección de objetos y análisis de imágenes, utilizando herramientas de aprendizaje profundo.\n",
    "\n",
    "#### 1. **Librerías estándar de Python**\n",
    "   - **`os` y `sys`**: Estas bibliotecas proporcionan herramientas para interactuar con el sistema operativo y manejar variables del entorno, como rutas de archivos y configuración del sistema.\n",
    "   - **`time`**: Se utiliza para añadir retrasos en la ejecución del código, lo cual puede ser útil para sincronizar procesos o medir tiempos de ejecución.\n",
    "   - **`collections.defaultdict`**: Una estructura de datos que proporciona un valor predeterminado para las claves que no existen en un diccionario.\n",
    "   - **`math`**: Biblioteca estándar para realizar cálculos matemáticos avanzados, como trigonometría y funciones algebraicas.\n",
    "\n",
    "#### 2. **Librerías de procesamiento de video e imágenes**\n",
    "   - **`cv2 (OpenCV)`**: Biblioteca de procesamiento de imágenes y videos ampliamente utilizada. Se usa para capturar, modificar, y analizar frames de video, como la detección de bordes, conversión a escala de grises, y la visualización de resultados.\n",
    "   - **`numpy (np)`**: Biblioteca para operaciones matemáticas y manipulación de arrays multidimensionales. Es fundamental para manejar datos de imágenes y videos en forma de matrices.\n",
    "   - **`moviepy.editor`**: Herramienta para la edición y manipulación de videos. Facilita la combinación, edición y exportación de videos.\n",
    "\n",
    "#### 3. **Librerías de PyTorch**\n",
    "   - **`torch`**: PyTorch es una biblioteca de aprendizaje profundo que se usa para construir y ejecutar modelos de redes neuronales. En este caso, se utiliza para la ejecución de modelos preentrenados.\n",
    "   - **`torchvision.transforms`**: Proporciona transformaciones para preprocesar imágenes, como redimensionar, recortar y normalizar las imágenes antes de que sean ingresadas a un modelo.\n",
    "   - **`torchvision.models`**: Ofrece varios modelos preentrenados para la visión por computadora, como ResNet, que se utiliza en este proyecto para la detección de puntos clave.\n",
    "\n",
    "#### 4. **Librerías de YOLO y Ultralytics**\n",
    "   - **`ultralytics.YOLO`**: YOLO (You Only Look Once) es un modelo de detección de objetos en tiempo real. La implementación de Ultralytics facilita la carga y uso de modelos YOLO preentrenados para detectar jugadores y objetos en la cancha de tenis.\n",
    "   - **`ultralytics.utils.plotting`**: Incluye herramientas para anotar (dibujar) en las imágenes, como los cuadros delimitadores y la coloración de los objetos detectados.\n",
    "\n",
    "#### 5. **Librerías de SORT para el seguimiento de objetos**\n",
    "   - **`sort.Sort`**: SORT (Simple Online and Realtime Tracking) es un algoritmo utilizado para hacer seguimiento de objetos en videos. En este proyecto, se utiliza para rastrear a los jugadores a lo largo de los frames del video, manteniendo el mismo ID de jugador incluso si la detección es momentáneamente perdida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías estándar de Python\n",
    "import os\n",
    "import sys\n",
    "import time  # Para añadir retrasos\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Librerías de procesamiento de video e imágenes\n",
    "import cv2\n",
    "import numpy as np\n",
    "import moviepy.editor as mpe\n",
    "\n",
    "# Librerías de PyTorch\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "# Librerías de YOLO y ultralytics\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "import ultralytics\n",
    "\n",
    "# Librerías de SORT para seguimiento de objetos\n",
    "from sort import Sort\n",
    "\n",
    "# Librerías de TensorFlow (aunque no está claro si se está utilizando en este contexto)\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción paso a paso de la clase `PlayerDetection`\n",
    "\n",
    "#### 1. **Constructor (`__init__`)**\n",
    "   - **Objetivo**: Inicializa la clase con los parámetros necesarios para procesar un video, detectar jugadores y analizar si están en la zona de saque.\n",
    "   - **Parámetros**:\n",
    "     - `video_path`: La ruta del archivo de video que se va a procesar.\n",
    "     - `yolo_model_path`: Ruta del modelo YOLO utilizado para la detección de jugadores.\n",
    "     - `pose_model_path`: Ruta del modelo YOLO utilizado para la detección de poses.\n",
    "     - `output_path`: La ruta donde se guardará el video procesado (opcional).\n",
    "   - **Variables**:\n",
    "     - `self.model`: Carga el modelo YOLO para detectar a los jugadores.\n",
    "     - `self.pose_model`: Carga un modelo YOLO especializado en detectar poses.\n",
    "     - `self.tracker`: Inicializa el algoritmo SORT para hacer seguimiento de los jugadores.\n",
    "     - `self.player_mapping`: Un diccionario para almacenar el ID de cada jugador.\n",
    "     - `self.threshold_y`: Umbral que se calcula para identificar la zona de saque.\n",
    "\n",
    "#### 2. **Método `process_video`**\n",
    "   - **Objetivo**: Procesar el video, detectar jugadores y analizar su relación con las zonas de saque.\n",
    "   - **Funcionamiento**:\n",
    "     - Carga el video y establece sus dimensiones.\n",
    "     - Actualiza los puntos clave de la cancha con el método `update_court_keypoints`.\n",
    "     - Utiliza YOLO para detectar jugadores en cada frame.\n",
    "     - Almacena las detecciones y usa el algoritmo SORT para hacer el seguimiento continuo de los jugadores.\n",
    "     - Si se detectan dos jugadores, se les asignan IDs y se analiza si están en la zona de saque usando los puntos clave detectados por el modelo de pose.\n",
    "\n",
    "#### 3. **Método `update_court_keypoints`**\n",
    "   - **Objetivo**: Actualiza los puntos clave de la cancha (keypoints) y calcula el umbral Y (`self.threshold_y`) que se utiliza para verificar si un jugador está en la zona de saque.\n",
    "   - **Funcionamiento**:\n",
    "     - Se predicen los puntos clave de la cancha usando `line_detection`.\n",
    "     - Calcula la angulación de la cancha y ajusta el umbral Y dependiendo de la diferencia angular entre las líneas detectadas.\n",
    "\n",
    "#### 4. **Método `calculate_angle`**\n",
    "   - **Objetivo**: Calcula el ángulo entre dos puntos de la cancha, lo que permite analizar la inclinación de las líneas laterales.\n",
    "   - **Parámetros**:\n",
    "     - `x1, y1`: Coordenadas del primer punto.\n",
    "     - `x2, y2`: Coordenadas del segundo punto.\n",
    "   - **Funcionamiento**:\n",
    "     - Calcula la diferencia en las coordenadas X e Y y utiliza `atan2` para calcular el ángulo entre los dos puntos.\n",
    "\n",
    "#### 5. **Método `detect_pose`**\n",
    "   - **Objetivo**: Detectar la pose del jugador en el frame actual y devolver las coordenadas de los tobillos izquierdo y derecho.\n",
    "   - **Funcionamiento**:\n",
    "     - Recorta el área del jugador en el frame.\n",
    "     - Usa el modelo de poses para predecir las coordenadas de los puntos clave del cuerpo.\n",
    "     - Ajusta las posiciones de los tobillos con un factor de corrección y devuelve las coordenadas de los tobillos y los puntos clave.\n",
    "\n",
    "#### 6. **Método `draw_keypoints`**\n",
    "   - **Objetivo**: Dibuja los puntos clave detectados sobre el frame actual.\n",
    "   - **Funcionamiento**:\n",
    "     - Recorre los puntos clave y los dibuja sobre el frame usando círculos y etiquetas.\n",
    "\n",
    "#### 7. **Método `detect_serve_zone`**\n",
    "   - **Objetivo**: Verificar si los tobillos del jugador están dentro de las zonas de saque.\n",
    "   - **Funcionamiento**:\n",
    "     - Define los polígonos que representan las zonas de saque para ambos jugadores.\n",
    "     - Verifica si los tobillos del jugador están dentro de los polígonos usando el método `is_inside_polygon`.\n",
    "     - Si un tobillo está dentro de la zona de saque, la zona se resalta en rojo.\n",
    "\n",
    "#### 8. **Método `highlight_polygon`**\n",
    "   - **Objetivo**: Resalta una zona en la cancha dibujando un polígono sobre el video.\n",
    "   - **Parámetros**:\n",
    "     - `polygon`: Array de puntos que define el polígono.\n",
    "     - `color`: El color que se utilizará para resaltar la zona (por defecto verde).\n",
    "\n",
    "#### 9. **Método `is_inside_polygon`**\n",
    "   - **Objetivo**: Verificar si un punto está dentro de un polígono.\n",
    "   - **Funcionamiento**:\n",
    "     - Usa `cv2.pointPolygonTest` para determinar si un punto está dentro del área definida por el polígono.\n",
    "\n",
    "#### 10. **Método `assign_player_ids`**\n",
    "   - **Objetivo**: Asignar IDs a los jugadores en función de su posición en la cancha.\n",
    "   - **Funcionamiento**:\n",
    "     - Si un jugador está por encima del umbral Y calculado, se le asigna como jugador 1; si está por debajo, como jugador 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_names = [\n",
    "    \"Left-shoulder\", \"Right-shoulder\", \"Left-elbow\", \"Right-elbow\",\n",
    "    \"Left-hip\", \"Right-hip\", \"Left-knee\", \"Right-knee\", \"Left-ankle\", \"Right-ankle\"\n",
    "]\n",
    "\n",
    "class PlayerDetection:\n",
    "    def __init__(self, video_path, yolo_model_path=\"bestv3.pt\", pose_model_path=\"yolo11x-pose.pt\", output_path=\"resultado_con_jugadores.mp4\"):\n",
    "        self.video_path = video_path\n",
    "        self.output_path = output_path\n",
    "        self.model = YOLO(yolo_model_path)  # Modelo para detectar jugadores (bestv3.pt)\n",
    "        self.pose_model = YOLO(pose_model_path)  # Modelo para detectar poses\n",
    "        self.tracker = Sort()\n",
    "        self.player_mapping = {}\n",
    "        self.threshold_y = None  # Umbral Y calculado\n",
    "        self.angle_left = None  # Angulación de la cancha lado izquierdo\n",
    "        self.angle_right = None  # Angulación de la cancha lado derecho\n",
    "\n",
    "    def process_video(self, line_detection):\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error al abrir el video.\")\n",
    "            return\n",
    "\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(self.output_path, fourcc, fps, (width, height))\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Actualiza puntos de la cancha\n",
    "        line_detection.predict_keypoints(frame)\n",
    "        self.update_court_keypoints(line_detection, frame)\n",
    "\n",
    "        while ret:\n",
    "            self.frame = frame\n",
    "\n",
    "            # Realiza predicciones con el modelo YOLO\n",
    "            results = self.model.predict(frame, conf=0.4, iou=0.3)  # Puede devolver una lista de detecciones\n",
    "\n",
    "            detections = []\n",
    "            for result in results:  # Procesamos cada predicción\n",
    "                boxes = result.boxes  # Extraemos el objeto Boxes\n",
    "                for box in boxes:\n",
    "                    # Extraemos las coordenadas xyxy, la clase y la confianza\n",
    "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)  # Coordenadas de la caja\n",
    "                    class_id = int(box.cls[0].cpu().numpy())  # Clase de la detección\n",
    "                    confidence = float(box.conf[0].cpu().numpy())  # Confianza de la detección\n",
    "\n",
    "                    if class_id == 3:  # Filtrar la clase 'player' con class_id=3\n",
    "                        print(f\"Jugador detectado con confianza {confidence} en posición: ({x1}, {y1}, {x2}, {y2})\")\n",
    "                        detections.append([x1, y1, x2, y2, confidence])\n",
    "\n",
    "            detections = np.array(detections)\n",
    "\n",
    "            if len(detections) == 0:\n",
    "                print(\"No se detectaron jugadores en este frame.\")\n",
    "            else:\n",
    "                print(f\"{len(detections)} jugadores detectados en este frame.\")\n",
    "\n",
    "            tracked_objects = self.tracker.update(detections)\n",
    "\n",
    "            player_positions = []\n",
    "            for track in tracked_objects:\n",
    "                x1, y1, x2, y2, track_id = map(int, track[:5])\n",
    "                player_positions.append((x2, y2))\n",
    "                # Dibuja las cajas delimitadoras\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f'ID: {track_id}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            if self.threshold_y is not None:\n",
    "                cv2.line(frame, (0, int(self.threshold_y)), (width, int(self.threshold_y)), (255, 0, 0), 2)\n",
    "\n",
    "            if len(player_positions) == 2:\n",
    "                player_1, player_2 = self.assign_player_ids(player_positions)\n",
    "\n",
    "                for i, (x, y) in enumerate(player_positions):\n",
    "                    player_label = \"jugador 1\" if (x, y) == player_1 else \"jugador 2\"\n",
    "                    x1, y1, x2, y2, track_id = map(int, tracked_objects[i][:5])\n",
    "\n",
    "                    left_ankle, right_ankle, keypoints = self.detect_pose((x1, y1, x2, y2), frame)\n",
    "\n",
    "                    if keypoints:\n",
    "                        self.detect_serve_zone(player_positions, left_ankle, right_ankle, line_detection.keypoints)\n",
    "                        self.draw_keypoints(frame, keypoints, (x1, y1, x2, y2))\n",
    "                        print(f\"Tobillo izquierdo: {left_ankle}, Tobillo derecho: {right_ankle}\")\n",
    "\n",
    "                    # Dibuja caja delimitadora y etiqueta del jugador\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, player_label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            out.write(frame)  # Escribe el frame con las detecciones en el video\n",
    "\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Video con jugadores guardado como {self.output_path}\")\n",
    "\n",
    "    def update_court_keypoints(self, line_detection, frame):\n",
    "        keypoints = line_detection.predict_keypoints(frame)\n",
    "\n",
    "        # Coordenadas de los puntos (0, 2) y (1, 3)\n",
    "        x0, y0 = keypoints[0], keypoints[1]\n",
    "        x2, y2 = keypoints[4], keypoints[5]\n",
    "        x1, y1 = keypoints[2], keypoints[3]\n",
    "        x3, y3 = keypoints[6], keypoints[7]\n",
    "\n",
    "        self.angle_left = self.calculate_angle(x0, y0, x2, y2)\n",
    "        self.angle_right = self.calculate_angle(x1, y1, x3, y3)\n",
    "\n",
    "        angle_difference = abs(self.angle_left - self.angle_right)\n",
    "        cateto_x_left = abs(x2 - x0)\n",
    "        cateto_x_right = abs(x3 - x1)\n",
    "        avg_cateto_x = (cateto_x_left + cateto_x_right) / 2\n",
    "        self.threshold_y = y0 + (avg_cateto_x / 2)\n",
    "\n",
    "        # Ajustes adicionales basados en la diferencia angular\n",
    "        if angle_difference < 10:\n",
    "            self.threshold_y += 275\n",
    "        elif 10 <= angle_difference < 20:\n",
    "            self.threshold_y += 230\n",
    "        elif 20 <= angle_difference < 30:\n",
    "            self.threshold_y += 185\n",
    "        elif 30 <= angle_difference < 40:\n",
    "            self.threshold_y += 140\n",
    "        elif 40 <= angle_difference < 50:\n",
    "            self.threshold_y += 95\n",
    "        elif 50 <= angle_difference < 60:\n",
    "            self.threshold_y += 50\n",
    "        else:\n",
    "            self.threshold_y += 45\n",
    "\n",
    "    def calculate_angle(self, x1, y1, x2, y2):\n",
    "        delta_y = y2 - y1\n",
    "        delta_x = x2 - x1\n",
    "        angle = math.degrees(math.atan2(delta_y, delta_x))\n",
    "        return angle\n",
    "\n",
    "    def detect_pose(self, player_box, frame, scale_factor=1.5, ankle_adjustment_factor=0.05):\n",
    "        x1, y1, x2, y2 = map(int, player_box)\n",
    "\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        new_width = int(width * scale_factor)\n",
    "        new_height = int(height * scale_factor)\n",
    "\n",
    "        x1 = max(0, x1 - (new_width - width) // 2)\n",
    "        y1 = max(0, y1 - (new_height - height) // 2)\n",
    "        x2 = min(frame.shape[1], x1 + new_width)\n",
    "        y2 = min(frame.shape[0], y1 + new_height)\n",
    "\n",
    "        cropped_frame = frame[y1:y2, x1:x2]\n",
    "\n",
    "        results = self.pose_model(cropped_frame)\n",
    "\n",
    "        if hasattr(results[0], 'keypoints'):\n",
    "            keypoints = results[0].keypoints.xyn.cpu().numpy()[0]\n",
    "\n",
    "            keypoints_scaled = []\n",
    "            for i, keypoint in enumerate(keypoints[5:]):  # Solo desde el cuello hacia abajo (5 en adelante)\n",
    "                if i not in [4, 5]:  # Ignorar las muñecas\n",
    "                    x_norm, y_norm = keypoint\n",
    "                    x_scaled = int(x_norm * cropped_frame.shape[1]) + x1\n",
    "                    y_scaled = int(y_norm * cropped_frame.shape[0]) + y1\n",
    "\n",
    "                    if x1 <= x_scaled <= x2 and y1 <= y2:\n",
    "                        keypoints_scaled.append((x_scaled, y_scaled))\n",
    "\n",
    "            if len(keypoints_scaled) >= 10:\n",
    "                left_ankle = keypoints_scaled[8]\n",
    "                right_ankle = keypoints_scaled[9]\n",
    "                height_player = y2 - y1\n",
    "\n",
    "                left_ankle = (left_ankle[0], left_ankle[1] + int(height_player * ankle_adjustment_factor))\n",
    "                right_ankle = (right_ankle[0], right_ankle[1] + int(height_player * ankle_adjustment_factor))\n",
    "\n",
    "                return left_ankle, right_ankle, keypoints_scaled\n",
    "            else:\n",
    "                print(f\"No se detectaron suficientes keypoints: {len(keypoints_scaled)} detectados.\")\n",
    "                return None, None, None\n",
    "        else:\n",
    "            print(\"No se encontraron keypoints en los resultados.\")\n",
    "            return None, None, None\n",
    "\n",
    "    def draw_keypoints(self, frame, keypoints, player_box):\n",
    "        \"\"\"\n",
    "        Dibuja los keypoints en el frame solo si están dentro de la caja delimitadora del jugador.\n",
    "        \"\"\"\n",
    "        x1, y1, x2, y2 = map(int, player_box)\n",
    "\n",
    "        for i, (x, y) in enumerate(keypoints):\n",
    "            if x1 <= x <= x2 and y1 <= y2:\n",
    "                if i < len(keypoint_names):\n",
    "                    cv2.circle(frame, (x, y), 5, (0, 0, 255), -1)\n",
    "                    cv2.putText(frame, keypoint_names[i], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            else:\n",
    "                print(f\"El keypoint '{keypoint_names[i]}' está fuera del área del jugador, no se dibujará.\")\n",
    "\n",
    "    def detect_serve_zone(self, player_positions, left_ankle, right_ankle, keypoints):\n",
    "        keypoints_zona_jugador_1 = [\n",
    "            (keypoints[8], keypoints[9]), (keypoints[12], keypoints[13]),\n",
    "            (keypoints[18], keypoints[19]), (keypoints[16], keypoints[17])\n",
    "        ]\n",
    "        keypoints_zona_jugador_2 = [\n",
    "            (keypoints[20], keypoints[21]), (keypoints[22], keypoints[23]),\n",
    "            (keypoints[14], keypoints[15]), (keypoints[10], keypoints[11])\n",
    "        ]\n",
    "\n",
    "        zona_jugador_1 = np.array(keypoints_zona_jugador_1, np.int32).reshape((-1, 1, 2))\n",
    "        zona_jugador_2 = np.array(keypoints_zona_jugador_2, np.int32).reshape((-1, 1, 2))\n",
    "\n",
    "        if left_ankle or right_ankle:\n",
    "            if self.is_inside_polygon(left_ankle, zona_jugador_1) or self.is_inside_polygon(right_ankle, zona_jugador_1):\n",
    "                self.highlight_polygon(zona_jugador_1, color=(0, 0, 255))\n",
    "                print(f\"Tobillo izquierdo: {left_ankle}, Tobillo derecho: {right_ankle} en la zona del Jugador 1\")\n",
    "\n",
    "            if self.is_inside_polygon(left_ankle, zona_jugador_2) or self.is_inside_polygon(right_ankle, zona_jugador_2):\n",
    "                self.highlight_polygon(zona_jugador_2, color=(0, 0, 255))\n",
    "                print(f\"Tobillo izquierdo: {left_ankle}, Tobillo derecho: {right_ankle} en la zona del Jugador 2\")\n",
    "\n",
    "    def highlight_polygon(self, polygon, color=(0, 255, 0)):\n",
    "        cv2.fillPoly(self.frame, [polygon], color)\n",
    "\n",
    "    def is_inside_polygon(self, point, polygon):\n",
    "        return cv2.pointPolygonTest(polygon, point, False) >= 0\n",
    "\n",
    "    def assign_player_ids(self, player_positions):\n",
    "        player_1 = None\n",
    "        player_2 = None\n",
    "\n",
    "        for i, (x, y) in enumerate(player_positions):\n",
    "            if y < self.threshold_y:\n",
    "                player_1 = (x, y)\n",
    "            else:\n",
    "                player_2 = (x, y)\n",
    "\n",
    "        return player_1, player_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción paso a paso de la clase `LineDetection`\n",
    "\n",
    "#### 1. **Constructor (`__init__`)**\n",
    "   - **Objetivo**: Inicializar la clase para procesar un video y detectar las líneas de la cancha utilizando puntos clave (keypoints).\n",
    "   - **Parámetros**:\n",
    "     - `video_path`: La ruta del video a procesar.\n",
    "     - `model_path`: La ruta del modelo preentrenado que se usará para detectar los puntos clave.\n",
    "     - `output_path`: La ruta donde se guardará el video procesado (opcional, por defecto \"resultado_con_lineas.mp4\").\n",
    "   - **Variables**:\n",
    "     - `self.device`: Detecta si hay disponible una GPU (CUDA) y, en caso contrario, usa la CPU.\n",
    "     - `self.model`: Carga un modelo ResNet101 preentrenado y lo ajusta para predecir 14 puntos clave (28 coordenadas).\n",
    "     - `self.transform`: Define las transformaciones necesarias para redimensionar y normalizar las imágenes antes de pasarlas al modelo.\n",
    "     - `self.keypoints`: Almacena los puntos clave predichos para su posterior uso.\n",
    "\n",
    "#### 2. **Método `predict_keypoints`**\n",
    "   - **Objetivo**: Predecir los puntos clave (keypoints) en un frame del video utilizando el modelo entrenado.\n",
    "   - **Funcionamiento**:\n",
    "     - Convierte la imagen de BGR (formato de OpenCV) a RGB.\n",
    "     - Aplica las transformaciones necesarias (redimensionar y normalizar).\n",
    "     - Utiliza el modelo para predecir los puntos clave en la imagen y los escala a la resolución original del video.\n",
    "\n",
    "#### 3. **Método `get_central_keypoints`**\n",
    "   - **Objetivo**: Devuelve los puntos centrales superior e inferior de la cancha.\n",
    "   - **Funcionamiento**:\n",
    "     - Extrae el punto superior central (índice 8) y el punto inferior central (índice 12) de los puntos clave predichos.\n",
    "\n",
    "#### 4. **Método `draw_keypoint_lines`**\n",
    "   - **Objetivo**: Dibujar líneas de conexión entre los puntos clave sobre el frame del video.\n",
    "   - **Funcionamiento**:\n",
    "     - Se define una lista de pares de puntos que deben conectarse entre sí mediante líneas.\n",
    "     - Para cada par de puntos, se dibuja una línea azul entre ellos si están dentro de los límites de la imagen.\n",
    "\n",
    "#### 5. **Método `draw_keypoints`**\n",
    "   - **Objetivo**: Dibujar los puntos clave detectados sobre el frame del video.\n",
    "   - **Funcionamiento**:\n",
    "     - Por cada par de coordenadas `x, y`, se dibuja un círculo verde en la imagen para marcar el punto clave.\n",
    "     - Cada punto también se etiqueta con su número de identificación (ID).\n",
    "\n",
    "#### 6. **Método `process_video`**\n",
    "   - **Objetivo**: Procesar el video cuadro por cuadro, detectar los puntos clave y dibujar las líneas y puntos detectados.\n",
    "   - **Funcionamiento**:\n",
    "     - Carga el video y configura las propiedades como la resolución y el número de frames por segundo.\n",
    "     - Para cada frame:\n",
    "       1. Convierte el frame a escala de grises.\n",
    "       2. Aplica un suavizado y binarización.\n",
    "       3. Realiza la detección de bordes.\n",
    "       4. Utiliza el modelo para predecir los puntos clave y los dibuja sobre el frame.\n",
    "     - Guarda el video procesado con las líneas y puntos detectados resaltados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineDetection:\n",
    "    def __init__(self, video_path, model_path, output_path=\"resultado_con_lineas.mp4\"):\n",
    "        self.video_path = video_path\n",
    "        self.output_path = output_path\n",
    "        self.model_path = model_path\n",
    "\n",
    "        # Definir el dispositivo (GPU si está disponible, de lo contrario CPU)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"GPU detectada. Se utilizará CUDA para la aceleración.\")\n",
    "        else:\n",
    "            print(\"No se detectó GPU. Se utilizará CPU.\")\n",
    "\n",
    "        # Cargar modelo preentrenado y mover al dispositivo\n",
    "        self.model = models.resnet101(pretrained=True)\n",
    "        self.model.fc = torch.nn.Linear(self.model.fc.in_features, 14 * 2)  # 14 puntos clave (x, y)\n",
    "        self.model.load_state_dict(torch.load(self.model_path, map_location=self.device))\n",
    "        self.model.to(self.device)  # Mover el modelo al dispositivo\n",
    "        self.model.eval()  # Coloca el modelo en modo evaluación (no entrenamiento)\n",
    "\n",
    "        # Transformaciones necesarias para el modelo\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),  # El modelo espera una imagen de 224x224\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # Atributo para almacenar los keypoints\n",
    "        self.keypoints = None\n",
    "\n",
    "    def predict_keypoints(self, image):\n",
    "        # Convertir la imagen a RGB (cv2 usa BGR)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Aplicar las transformaciones y agregar una dimensión (batch size)\n",
    "        image_tensor = self.transform(image_rgb).unsqueeze(0).to(self.device)  # Mover la imagen al dispositivo\n",
    "\n",
    "        # Desactivar la retropropagación y obtener predicciones\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image_tensor)\n",
    "\n",
    "        # Convertir las predicciones a un formato usable (numpy array)\n",
    "        keypoints = outputs.squeeze().cpu().numpy()  # Mover a CPU antes de convertir a numpy\n",
    "\n",
    "        # Obtener el tamaño original del video\n",
    "        original_h, original_w = image.shape[:2]\n",
    "        # Escalar las coordenadas predichas desde 224x224 (modelo) a la resolución original del video\n",
    "        keypoints[0::2] = keypoints[0::2] * original_w / 224  # Escalar las coordenadas x\n",
    "        keypoints[1::2] = keypoints[1::2] * original_h / 224  # Escalar las coordenadas y\n",
    "        self.keypoints = keypoints\n",
    "        return keypoints\n",
    "\n",
    "    def get_central_keypoints(self, keypoints):\n",
    "        \"\"\"\n",
    "        Devuelve los puntos centrales superiores e inferiores.\n",
    "        Supongamos que los puntos clave están en el orden [x1, y1, x2, y2, ..., x14, y14].\n",
    "        \"\"\"\n",
    "        central_top = (keypoints[8], keypoints[9])     # Ejemplo de punto central superior\n",
    "        central_bottom = (keypoints[12], keypoints[13])  # Ejemplo de punto central inferior\n",
    "        return central_top, central_bottom\n",
    "\n",
    "    def draw_keypoint_lines(self, image, keypoints):\n",
    "        # Dibujar líneas de conexión entre los puntos clave\n",
    "        connections = [\n",
    "            (0, 1), (2, 3), (4, 6), (5, 7), (8, 9), (10, 11), (12, 13),\n",
    "            (0, 2), (1, 3), (4, 5), (6, 7), (8, 10), (9, 11)\n",
    "        ]\n",
    "\n",
    "        for (i, j) in connections:\n",
    "            x1, y1 = int(keypoints[i * 2]), int(keypoints[i * 2 + 1])\n",
    "            x2, y2 = int(keypoints[j * 2]), int(keypoints[j * 2 + 1])\n",
    "\n",
    "            if (0 <= x1 < image.shape[1] and 0 <= y1 < image.shape[0] and\n",
    "                0 <= x2 < image.shape[1] and 0 <= y2 < image.shape[0]):\n",
    "                cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Línea azul\n",
    "        return image\n",
    "\n",
    "    def draw_keypoints(self, image, keypoints):\n",
    "        # Dibujar puntos clave y su ID\n",
    "        for i in range(0, len(keypoints), 2):\n",
    "            x = int(keypoints[i])\n",
    "            y = int(keypoints[i + 1])\n",
    "\n",
    "            if 0 <= x < image.shape[1] and 0 <= y < image.shape[0]:\n",
    "                cv2.circle(image, (x, y), 10, (0, 255, 0), -1)  # Dibujar puntos verdes\n",
    "                cv2.putText(image, str(i // 2), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 0), 2)  # Mostrar ID\n",
    "        return image\n",
    "\n",
    "    def process_video(self, kernel_size_dilate=7, dilate_iterations=1, kernel_size_blur=5):\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"No se pudo abrir el video: {self.video_path}\")\n",
    "            return\n",
    "\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(self.output_path, fourcc, fps, (width, height))\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        while ret:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Suavizado y binarización\n",
    "            smoothed = cv2.GaussianBlur(gray, (kernel_size_blur, kernel_size_blur), 0)\n",
    "            _, binary = cv2.threshold(smoothed, 200, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # Dilatación y detección de bordes\n",
    "            kernel = np.ones((kernel_size_dilate, kernel_size_dilate), np.uint8)\n",
    "            dilated = cv2.dilate(binary, kernel, iterations=dilate_iterations)\n",
    "            edges = cv2.Canny(dilated, 50, 150)\n",
    "\n",
    "            # Detección de puntos clave y dibujo de líneas y puntos\n",
    "            keypoints = self.predict_keypoints(frame)\n",
    "            frame = self.draw_keypoints(frame, keypoints)\n",
    "            frame = self.draw_keypoint_lines(frame, keypoints)\n",
    "\n",
    "            out.write(frame)\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Video procesado y guardado en {self.output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción paso a paso de la clase `VideoMerger`\n",
    "\n",
    "#### 1. **Constructor (`__init__`)**\n",
    "   - **Objetivo**: Inicializar la clase con las rutas de los videos y definir las rutas de salida.\n",
    "   - **Parámetros**:\n",
    "     - `player_video_path`: Ruta del video que contiene la detección de jugadores.\n",
    "     - `line_video_path`: Ruta del video que contiene la detección de las líneas.\n",
    "     - `output_path`: Ruta donde se guardará el video final (opcional, por defecto \"video_final.mp4\").\n",
    "     - `original_video_path`: Ruta del video original con audio que se utilizará para añadir el audio al video final.\n",
    "   \n",
    "#### 2. **Método `merge_videos`**\n",
    "   - **Objetivo**: Combinar los videos de la detección de jugadores y líneas, y luego añadir el audio del video original.\n",
    "   - **Funcionamiento**:\n",
    "     - Abre los videos de detección de jugadores y líneas con OpenCV.\n",
    "     - Verifica que ambos videos se hayan abierto correctamente.\n",
    "     - Crea un archivo de video temporal donde se combinarán ambos videos.\n",
    "     - Superpone los frames de los dos videos utilizando la función `cv2.addWeighted` con un peso de 0.5 para cada video.\n",
    "     - Guarda el video combinado en un archivo temporal.\n",
    "     - Llama al método `add_audio_to_video` para añadir el audio del video original al video combinado.\n",
    "\n",
    "#### 3. **Método `add_audio_to_video`**\n",
    "   - **Objetivo**: Añadir el audio del video original al video final que se generó al combinar los videos.\n",
    "   - **Funcionamiento**:\n",
    "     - Utiliza la biblioteca `moviepy` para cargar el video combinado y el video original.\n",
    "     - Extrae el audio del video original y lo añade al video combinado.\n",
    "     - Guarda el video final con el audio en el archivo de salida definido.\n",
    "\n",
    "---\n",
    "\n",
    "### Descripción de la función `process_videos`\n",
    "\n",
    "#### **Objetivo**: Procesar una lista de videos, realizar detección de líneas y jugadores, combinarlos y luego añadir el audio del video original.\n",
    "\n",
    "#### **Funcionamiento**:\n",
    "   1. Para cada video en la lista `video_names`, se imprime un mensaje indicando que se está procesando.\n",
    "   2. **Generación de nombres de salida**:\n",
    "      - Se crea un nombre único para cada archivo de salida basado en el nombre del video de entrada:\n",
    "        - `player_output`: Video con la detección de jugadores.\n",
    "        - `line_output`: Video con la detección de líneas.\n",
    "        - `merged_output`: Video final que combina ambos resultados.\n",
    "   3. **Detección de líneas**:\n",
    "      - Se inicializa un objeto `LineDetection` y se llama a su método `process_video()` para procesar la detección de líneas en la cancha de tenis.\n",
    "   4. **Detección de jugadores**:\n",
    "      - Se inicializa un objeto `PlayerDetection` y se pasa el detector de líneas para procesar la detección de jugadores y su relación con las líneas de la cancha.\n",
    "   5. **Combinación de videos**:\n",
    "      - Se utiliza la clase `VideoMerger` para combinar el video de la detección de jugadores con el de las líneas y generar un video final.\n",
    "   6. **Añadir audio**:\n",
    "      - El método `add_audio_to_video` añade el audio del video original al video final combinado.\n",
    "\n",
    "#### **Lista de videos a procesar**\n",
    "   - La lista `video_names` contiene los nombres de los videos a procesar. El código procesará cada uno de los videos, generando un video final con audio para cada entrada.\n",
    "\n",
    "#### Ejecución del código\n",
    "   - El último bloque del código llama a la función `process_videos()` con la lista `video_names` especificada, procesando todos los videos indicados en esa lista.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoMerger:\n",
    "    def __init__(self, player_video_path, line_video_path, output_path=\"video_final.mp4\", original_video_path=None):\n",
    "        self.player_video_path = player_video_path\n",
    "        self.line_video_path = line_video_path\n",
    "        self.output_path = output_path\n",
    "        self.original_video_path = original_video_path  # El video original con audio\n",
    "\n",
    "    def merge_videos(self):\n",
    "        cap1 = cv2.VideoCapture(self.player_video_path)\n",
    "        cap2 = cv2.VideoCapture(self.line_video_path)\n",
    "\n",
    "        if not cap1.isOpened() or not cap2.isOpened():\n",
    "            print(f\"Error al abrir los videos: {self.player_video_path} o {self.line_video_path}\")\n",
    "            return\n",
    "\n",
    "        width = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = int(cap1.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "        # Utilizamos 'XVID' para mayor compatibilidad\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        temp_video_path = \"temp_video_without_audio.mp4\"\n",
    "        out = cv2.VideoWriter(temp_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "        ret1, frame1 = cap1.read()\n",
    "        ret2, frame2 = cap2.read()\n",
    "\n",
    "        while ret1 and ret2:\n",
    "            combined_frame = cv2.addWeighted(frame1, 0.5, frame2, 0.5, 0)\n",
    "            out.write(combined_frame)\n",
    "\n",
    "            ret1, frame1 = cap1.read()\n",
    "            ret2, frame2 = cap2.read()\n",
    "\n",
    "        cap1.release()\n",
    "        cap2.release()\n",
    "        out.release()\n",
    "\n",
    "        print(f\"Video combinado guardado como {temp_video_path}, ahora agregando el audio...\")\n",
    "\n",
    "        # Usar moviepy para agregar el audio del video original\n",
    "        self.add_audio_to_video(temp_video_path, self.original_video_path, self.output_path)\n",
    "\n",
    "    def add_audio_to_video(self, video_path, audio_video_path, output_path):\n",
    "        \"\"\"\n",
    "        Agrega el audio del video original al video final renderizado.\n",
    "        \"\"\"\n",
    "        video_clip = mpe.VideoFileClip(video_path)\n",
    "        original_video = mpe.VideoFileClip(audio_video_path)\n",
    "        \n",
    "        # Combinar el video renderizado con el audio del video original\n",
    "        final_video = video_clip.set_audio(original_video.audio)\n",
    "        \n",
    "        # Guardar el video final con audio\n",
    "        final_video.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n",
    "        \n",
    "        print(f\"Video final con audio guardado como {output_path}\")\n",
    "\n",
    "def process_videos(video_names):\n",
    "    for video_name in video_names:\n",
    "        print(f\"Procesando el video: {video_name}\")\n",
    "\n",
    "        # Remover la extensión del archivo para crear nombres únicos\n",
    "        base_name = os.path.splitext(video_name)[0]\n",
    "\n",
    "        # Crear nombres únicos para los archivos de salida para cada video\n",
    "        player_output = f\"resultado_con_jugadores_{base_name}.mp4\"\n",
    "        line_output = f\"resultado_con_lineas_{base_name}.mp4\"\n",
    "        merged_output = f\"final_{base_name}.mp4\"\n",
    "\n",
    "        # Procesar detección de líneas blancas\n",
    "        detector = LineDetection(video_name, 'keypoints_model_v2.pth', output_path=line_output)\n",
    "        detector.process_video()\n",
    "\n",
    "        # Procesar detección de jugadores usando el detector de líneas\n",
    "        player_detection = PlayerDetection(video_name, output_path=player_output)\n",
    "        player_detection.process_video(detector)\n",
    "\n",
    "        # Combinar ambos resultados y añadir el audio del video original\n",
    "        video_merger = VideoMerger(player_output, line_output, merged_output, original_video_path=video_name)\n",
    "        video_merger.merge_videos()\n",
    "\n",
    "\n",
    "# Lista de los videos a procesar\n",
    "video_names = [\"input_video.mp4\",\"Untitled.mp4\",\"Untitled2.mp4\",\"Untitled32.mp4\"]\n",
    "#video_names = [\"Untitled32.mp4\"]\n",
    "# Procesar todos los videos\n",
    "process_videos(video_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
